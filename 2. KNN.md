# 第2章 K近邻算法

***

### 2.1 算法基础

***

其本质是当未知类别的多个样本的特征足够相似时，则他们更有可能同属一个类别。

```python
# 导入模块
import numpy as np
import matplotlib.pyplot as plt
# 导入数据
raw_data_X = [[3.393533211, 2.331273381],
              [3.110073483, 1.781539638],
              [1.343808831, 3.368360954],
              [3.582294042, 4.679179110],
              [2.280362439, 2.866990263],
              [7.423436942, 4.696522875],
              [5.745051997, 3.533989803],
              [9.172168622, 2.511101045],
              [7.792783481, 3.424088941],
              [7.939820817, 0.791637231]
             ]
raw_data_y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
X_train = np.array(raw_data_X)
y_train = np.array(raw_data_y)
# **画预测散点图**
x = np.array([8.093607318, 3.365731514])		# 预测点
plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], color='g')
plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], color='r')
plt.scatter(x[0], x[1], color='b')
plt.show()
## KNN过程
# **求每个点之间的距离**
from math import sqrt
distances = []		# 亦可distances = [sqrt(np.sum((x_train - x)**2)) for x_train in X_train]
for x_train in X_train:
    d = sqrt(np.sum((x_train - x)**2))
    distances.append(d)
# **对距离进行排序**
nearest = np.argsort(distances)		# 将索引从小到大进行排序
k = 6
topK_y = [y_train[neighbor] for neighbor in nearest[:k]]
# **投票**
from collections import Counter
votes = Counter(topK_y)		# Counter({0: 1, 1: 5})表示类型以及票数
votes.most_common(1)		# [(1, 5)]表类型1的票数为5
predict_y = votes.most_common(1)[0][0]
print('x预测类型为：', predict_y)
```

***

### 2.2 scikit-learn中的算法封装

***

#### 2.2.1 KNN算法封装

**（1）`kNN/kNN.py`文件**

```python
import numpy as np
from math import sqrt
from collections import Counter    # 计数器的引入用来投票


class KNNClassifier:

    def __init__(self, k):
        """初始化kNN分类器"""
        assert k >= 1, "k must be valid"
        self.k = k
        self._X_train = None
        self._y_train = None

    def fit(self, X_train, y_train):
        """根据训练数据集X_train和y_train训练kNN分类器"""
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"
        assert self.k <= X_train.shape[0], \
            "the size of X_train must be at least k."

        self._X_train = X_train
        self._y_train = y_train
        return self

    def predict(self, X_predict):
        """给定待预测数据集X_predict，返回表示X_predict的结果向量"""
        assert self._X_train is not None and self._y_train is not None, \
                "must fit before predict!"
        assert X_predict.shape[1] == self._X_train.shape[1], \
                "the feature number of X_predict must be equal to X_train"

        y_predict = [self._predict(x) for x in X_predict]
        return np.array(y_predict)

    def _predict(self, x):
        """给定单个待预测数据x，返回x的预测结果值"""
        assert x.shape[0] == self._X_train.shape[1], \
            "the feature number of x must be equal to X_train"

        distances = [sqrt(np.sum((x_train - x) ** 2))
                     for x_train in self._X_train]
        nearest = np.argsort(distances)

        topK_y = [self._y_train[i] for i in nearest[:self.k]]
        votes = Counter(topK_y)

        return votes.most_common(1)[0][0]

    def __repr__(self):
        return "KNN(k=%d)" % self.k
```

```Python
import numpy as np
import matplotlib.pyplot as plt
raw_data_X = [[3.393533211, 2.331273381],
              [3.110073483, 1.781539638],
              [1.343808831, 3.368360954],
              [3.582294042, 4.679179110],
              [2.280362439, 2.866990263],
              [7.423436942, 4.696522875],
              [5.745051997, 3.533989803],
              [9.172168622, 2.511101045],
              [7.792783481, 3.424088941],
              [7.939820817, 0.791637231]
             ]
raw_data_y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]

X_train = np.array(raw_data_X)
y_train = np.array(raw_data_y)

x = np.array([8.093607318, 3.365731514])
%run kNN/kNN.py
knn_clf = KNNClassifier(3)
knn_clf.fit(X_train, y_train)
y_predict = knn_clf.predict(X_predict)
y_predict[0]
```

**（2）`kNN_function/kNN.py`文件**

```Python
## kNN_function/kNN.py
import numpy as np
from math import sqrt
from collections import Counter
# KNN分类
def kNN_classify(k, X_train, y_train, x):
		"""首先使用断言保证输入数据的合法性，然后按照算法流程进行编程"""
    assert 1 <= k <= X_train.shape[0], "k must be valid"
    assert X_train.shape[0] == y_train.shape[0], \
        "the size of X_train must equal to the size of y_train"
    assert X_train.shape[1] == x.shape[0], \
        "the feature number of x must be equal to X_train"

    distances = [sqrt(np.sum((x_train - x)**2)) for x_train in X_train]
    nearest = np.argsort(distances)

    topK_y = [y_train[i] for i in nearest[:k]]
    votes = Counter(topK_y)

    return votes.most_common(1)[0][0]
```

```python
%run kNN_function/kNN.py
predict_y = kNN_classify(6, X_train, y_train, x)
predict_y
```

#### 2.2.2 scikit-learn的调用

```Python
# 加载scikit-learn中的相应算法
from sklearn.neighbors import KNeighborsClassifier
# 创建实例并传入相关参数
kNN_classifier = KNeighborsClassifier(n_neighbors=6)
# 用fit拟合数据集
kNN_classifier.fit(X_train, y_train)
# 用predict来预测
"""kNN_classifier.predict(x)为错误方式，此时应传入矩阵形式的参数，否则报错"""
X_predict = x.reshape(1, -1)
y_predict = kNN_classifier.predict(X_predict)    # 即可得到np矩阵形式的预测值
y_predict[0]    # 可得到所属类
```

***

### 2.3 判断算法的性能

***

#### 2.3.1 数据集的分割

将数据集分为训练集和测试集。注意：切分模型时应将原始数据进行随机化，且应注意label不能混淆。

**（1）算法实现**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets 

iris = datasets.load_iris()
X = iris.data
y = iris.target
# 分割训练集和测试集
shuffled_indexes = np.random.permutation(len(X))    # 对len(x)随机排序
test_ratio = 0.2
test_size = int(len(X) * test_ratio)
test_indexes = shuffled_indexes[:test_size]
train_indexes = shuffled_indexes[test_size:]
# 得到训练集和测试集
X_train = X[train_indexes]
y_train = y[train_indexes]

X_test = X[test_indexes]
y_test = y[test_indexes]
```

**（2）算法封装**

`playML/model_selection.py`文件

```Python
import numpy as np


def train_test_split(X, y, test_ratio=0.2, seed=None):
    """将数据 X 和 y 按照test_ratio分割成X_train, X_test, y_train, y_test"""
    assert X.shape[0] == y.shape[0], \
        "the size of X must be equal to the size of y"
    assert 0.0 <= test_ratio <= 1.0, \
        "test_ration must be valid"

    if seed:
        np.random.seed(seed)

    shuffled_indexes = np.random.permutation(len(X))

    test_size = int(len(X) * test_ratio)
    test_indexes = shuffled_indexes[:test_size]
    train_indexes = shuffled_indexes[test_size:]

    X_train = X[train_indexes]
    y_train = y[train_indexes]

    X_test = X[test_indexes]
    y_test = y[test_indexes]

    return X_train, X_test, y_train, y_test
```

导入算法：

```Python
from playML.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y)
```

**（3）sklearn中的train_test_split:**

```python 
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)
```

#### 2.3.2 分类准确性

**（1）具体实现**

```python
from playML.kNN import KNNClassifier    # playML/kNN.py 同 2.2.1 中的 KNN.py

my_knn_clf = KNNClassifier(k=3)
my_knn_clf.fit(X_train, y_train)
y_predict = my_knn_clf.predict(X_test)
accuracy_score = sum(y_predict == y_test) / len(y_test)
```

手写体数据集：

```Python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets    # 手写体数据集

digits = datasets.load_digits()
X = digits.data
y = digits.target
# 显示其中某一个图像
some_digit = X[666]
some_digit_image = some_digit.reshape(8, 8)

import matplotlib
import matplotlib.pyplot as plt
plt.imshow(some_digit_image, cmap = matplotlib.cm.binary)
plt.show()
# 计算KNN的准确性
from playML.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_ratio=0.2)

from playML.kNN import KNNClassifier    # 见2.3.1中的（2）
my_knn_clf = KNNClassifier(k=3)
my_knn_clf.fit(X_train, y_train)
y_predict = my_knn_clf.predict(X_test)
sum(y_predict == y_test) / len(y_test)
accuracy_score = sum(y_predict == y_test) / len(y_test)
```

**（2）封装代码**

```Python
import numpy as np


def accuracy_score(y_true, y_predict):
    '''计算y_true和y_predict之间的准确率'''
    assert y_true.shape[0] == y_predict.shape[0], \
        "the size of y_true must be equal to the size of y_predict"

    return sum(y_true == y_predict) / len(y_true)
```

调用自己的代码

```
from playML.metrics import accuracy_score

accuracy_score(y_test, y_predict)
```

**（3）scikit-learn的调用**

```python
from sklearn.model_selection import train_test_split    # 导入分割算法

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)

from sklearn.neighbors import KNeighborsClassifier    # 导入KNN算法

knn_clf = KNeighborsClassifier(n_neighbors=3)
knn_clf.fit(X_train, y_train)
y_predict = knn_clf.predict(X_test)

from sklearn.metrics import accuracy_score    # 导入准确性测试算法

accuracy_score(y_test, y_predict)
```

***

### 2.4 超参数的选择

***

```Python
import numpy as np
from sklearn import datasets
# 导入digital数据库
digits = datasets.load_digits()
X = digits.data
y = digits.target
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=666)
# 导入KNN算法
from sklearn.neighbors import KNeighborsClassifier

knn_clf = KNeighborsClassifier(n_neighbors=3)
knn_clf.fit(X_train, y_train)
knn_clf.score(X_test, y_test)
```

#### 2.4.1 超参数K的选择

```Python
# 寻找最好的K
best_score = 0.0
best_k = -1
for k in range(1, 11):
    knn_clf = KNeighborsClassifier(n_neighbors=k)
    knn_clf.fit(X_train, y_train)
    score = knn_clf.score(X_test, y_test)
    if score > best_score:
        best_k = k
        best_score = score
        
print("best_k =", best_k)
print("best_score =", best_score)
# 要不要考虑距离
best_score = 0.0
best_k = -1
best_method = ""
for method in ["uniform", "distance"]:
    for k in range(1, 11):
        knn_clf = KNeighborsClassifier(n_neighbors=k, weights=method)
        knn_clf.fit(X_train, y_train)
        score = knn_clf.score(X_test, y_test)
        if score > best_score:
            best_k = k
            best_score = score
            best_method = method
        
print("best_method =", best_method)
print("best_k =", best_k)
print("best_score =", best_score)
# 探索明可夫斯基距离相应的p
best_score = 0.0
best_k = -1
best_p = -1

for k in range(1, 11):
    for p in range(1, 6):
        knn_clf = KNeighborsClassifier(n_neighbors=k, weights="distance", p=p)
        knn_clf.fit(X_train, y_train)
        score = knn_clf.score(X_test, y_test)
        if score > best_score:
            best_k = k
            best_p = p
            best_score = score
        
print("best_k =", best_k)
print("best_p =", best_p)
print("best_score =", best_score)
```

#### 2.4.2 网格搜索和其它KNN超参数

```Python
# 网格搜索Grid Search
param_grid = [
    {
        'weights': ['uniform'], 
        'n_neighbors': [i for i in range(1, 11)]
    },
    {
        'weights': ['distance'],
        'n_neighbors': [i for i in range(1, 11)], 
        'p': [i for i in range(1, 6)]
    }
]
knn_clf = KNeighborsClassifier()
from sklearn.model_selection import GridSearchCV

grid_search = GridSearchCV(knn_clf, param_grid)

grid_search.best_estimator_    # 可获得最优解
grid_search.best_score_    # 可获得最好的准确度
```

`best_estimator_`:当不是用户传入的参数，而是根据用户的传入参数，类自己计算的结果命名时末尾加“_”

```Python
# n_jobs表示使用的核数，verbose数字越大展示的信息越详细
grid_search.fit(X_train, y_train)
grid_search = GridSearchCV(knn_clf, param_grid, n_jobs=-1, verbose=2)
```

***

### 2.5 数据归一化处理

***

#### 2.5.1 归一化处理

**（1）最值归一化**

```Python
import numpy as np
import matplotlib.pyplot as plt
# 一维
x = np.random.randint(0, 100, 100) 
(x - np.min(x)) / (np.max(x) - np.min(x))
# 二维
X = np.random.randint(0, 100, (50, 2))
X = np.array(X, dtype=float)
X[:,0] = (X[:,0] - np.min(X[:,0])) / (np.max(X[:,0]) - np.min(X[:,0]))
X[:,1] = (X[:,1] - np.min(X[:,1])) / (np.max(X[:,1]) - np.min(X[:,1]))
plt.scatter(X[:,0], X[:,1])    # 画散点图
plt.show()
np.mean(X[:,0])    # 计算均值
np.std(X[:,1])    # 计算标准差
```

**（2）均值方差归一化** 

```
X2[:,0] = (X2[:,0] - np.mean(X2[:,0])) / np.std(X2[:,0])
X2[:,1] = (X2[:,1] - np.mean(X2[:,1])) / np.std(X2[:,1])
```

#### 2.5.2 Scikit-learn中的Scaler

```Python
import numpy as np
from sklearn import datasets
iris = datasets.load_iris()
X = iris.data
y = iris.target

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=666)
```

**（1）scikit-learn中的StandardScaler**

```Python
from sklearn.preprocessing import StandardScaler 
standardScalar = StandardScaler()
standardScalar.fit(X_train)
print(standardScalar.mean_, standardScalar.scale_)
X_train = standardScalar.transform(X_train)
X_test_standard = standardScalar.transform(X_test) 
# 使用归一化之后的数据进行CNN分类
from sklearn.neighbors import KNeighborsClassifier
knn_clf = KNeighborsClassifier(n_neighbors=3)
knn_clf.fit(X_train, y_train)
knn_clf.score(X_test_standard, y_test)    # 注意：此时应该传入归一化之后的测试集
```

**（2）实现我们自己的StandardScaler**

`playML/preprocessing`文件：

```Python
import numpy as np


class StandardScaler:

    def __init__(self):
        self.mean_ = None
        self.scale_ = None

    def fit(self, X):
        """根据训练数据集X获得数据的均值和方差"""
        assert X.ndim == 2, "The dimension of X must be 2"

        self.mean_ = np.array([np.mean(X[:,i]) for i in range(X.shape[1])])
        self.scale_ = np.array([np.std(X[:,i]) for i in range(X.shape[1])])

        return self

    def transform(self, X):
        """将X根据这个StandardScaler进行均值方差归一化处理"""
        assert X.ndim == 2, "The dimension of X must be 2"
        assert self.mean_ is not None and self.scale_ is not None, \
               "must fit before transform!"
        assert X.shape[1] == len(self.mean_), \
               "the feature number of X must be equal to mean_ and std_"

        resX = np.empty(shape=X.shape, dtype=float)
        for col in range(X.shape[1]):
            resX[:,col] = (X[:,col] - self.mean_[col]) / self.scale_[col]
        return resX
```

```python 
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=666)
from playML.preprocessing import StandardScaler

my_standardScalar = StandardScaler() 
my_standardScalar.fit(X_train)
print(my_standardScalar.mean_, my_standardScalar.scale_)
X_train = standardScalar.transform(X_train)
```

***

### 2.6 小结

***

KNN缺点：

  * 效率低下
  * 高度数据不相关
  * 不具备可解释性
  * 维数灾难——可通过降为解决

